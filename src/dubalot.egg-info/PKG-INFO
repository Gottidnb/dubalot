Metadata-Version: 2.4
Name: dubalot
Version: 0.1.0
Summary: Translate video to a target language while preserving voice, background audio, and lip sync
License: MIT
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: moviepy>=1.0.3
Requires-Dist: openai-whisper>=20231117
Requires-Dist: deep-translator>=1.11.4
Requires-Dist: coqui-tts>=0.24.0
Requires-Dist: librosa>=0.10.1
Requires-Dist: soundfile>=0.12.1
Requires-Dist: pydub>=0.25.1
Requires-Dist: numpy>=1.24.0
Requires-Dist: scipy>=1.11.0
Requires-Dist: torch>=2.0.0
Requires-Dist: tqdm>=4.65.0
Provides-Extra: lipsync
Provides-Extra: source-separation
Requires-Dist: demucs>=4.0.0; extra == "source-separation"
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-mock>=3.10; extra == "dev"

# dubalot

> Translate foreign video to native language with the same voice.

**dubalot** is an end-to-end video translation pipeline that:

- ðŸŽ¤ **Preserves the speaker's voice** with a natural accent via XTTS v2 voice cloning
- ðŸŒ **Translates speech** to any language supported by Google Translate
- ðŸŽµ **Keeps background audio** (music, ambient noise) intact using Demucs source separation
- ðŸ‘„ **Syncs lips** with the translated audio via Wav2Lip (optional)
- â±ï¸ **Aligns timing** by time-stretching each translated segment to match the original

---

## Architecture

```
Source video
    â”‚
    â”œâ”€â–º [1] Extract audio (ffmpeg / MoviePy)
    â”‚
    â”œâ”€â–º [2] Separate vocals + background (Demucs)
    â”‚
    â”œâ”€â–º [3] Transcribe speech with timestamps (Whisper)
    â”‚
    â”œâ”€â–º [4] Translate segments (Google Translate via deep-translator)
    â”‚
    â”œâ”€â–º [5] Clone voice + synthesise TTS (Coqui XTTS v2)
    â”‚         â””â”€ Time-stretch each segment to original duration
    â”‚
    â”œâ”€â–º [6] Mix translated speech with background audio
    â”‚
    â”œâ”€â–º [7] Apply lip sync (Wav2Lip) â† optional
    â”‚
    â””â”€â–º Compose final video (ffmpeg)
```

---

## Installation

### Core dependencies

```bash
pip install -e .
# or
pip install -r requirements.txt
```

> Requires **Python â‰¥ 3.8** and **ffmpeg** on your `PATH`.

### Optional: source separation (highly recommended)

Background audio preservation is significantly improved when Demucs is installed:

```bash
pip install demucs
```

### Optional: lip sync (Wav2Lip)

Wav2Lip is not on PyPI and must be installed manually:

```bash
git clone https://github.com/Rudrabha/Wav2Lip
cd Wav2Lip
pip install -r requirements.txt
# Download the pretrained GAN checkpoint:
# https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/...
# Save it as: Wav2Lip/checkpoints/wav2lip_gan.pth

# Tell dubalot where to find it:
export WAV2LIP_DIR=/path/to/Wav2Lip
```

---

## Quick start

```bash
# Translate an English video to Spanish
dubalot --input video.mp4 --target-language es --output video_es.mp4

# Use a GPU and the large Whisper model for better accuracy
dubalot --input video.mp4 --target-language fr \
        --model large-v3 --device cuda \
        --output video_fr.mp4

# Provide a custom voice reference clip (3â€“30 s of clean speech)
dubalot --input video.mp4 --target-language de \
        --voice-reference speaker_ref.wav \
        --output video_de.mp4

# Skip lip sync (faster, no Wav2Lip required)
dubalot --input video.mp4 --target-language ja \
        --no-lip-sync --output video_ja.mp4
```

### Python API

```python
from dubalot import translate_video

translate_video(
    input_path="video.mp4",
    output_path="video_es.mp4",
    target_language="es",            # BCP-47 / Google Translate code
    source_language="auto",          # or e.g. "en"
    whisper_model="base",            # tiny | base | small | medium | large-v3
    lip_sync=True,                   # requires WAV2LIP_DIR
    device="cpu",                    # or "cuda"
)
```

---

## CLI reference

```
usage: dubalot [-h] -i PATH -o PATH -t LANG [-s LANG] [-m MODEL]
               [--voice-reference PATH] [--no-lip-sync] [--wav2lip-dir PATH]
               [--device {cpu,cuda}] [--keep-temp] [-v]

options:
  -i, --input PATH            Source video file (required)
  -o, --output PATH           Output video file (required)
  -t, --target-language LANG  Target language code, e.g. es, fr, de, ja (required)
  -s, --source-language LANG  Source language code or 'auto' (default: auto)
  -m, --model MODEL           Whisper model size (default: base)
  --voice-reference PATH      Custom speaker reference WAV (3â€“30 s)
  --no-lip-sync               Disable Wav2Lip lip synchronisation
  --wav2lip-dir PATH          Path to Wav2Lip repo (overrides WAV2LIP_DIR env var)
  --device {cpu,cuda}         PyTorch device (default: cpu)
  --keep-temp                 Keep temporary files for debugging
  -v, --verbose               Enable DEBUG logging
```

---

## Supported languages

dubalot supports all languages available in both **Google Translate** and
**Coqui XTTS v2**.  Common codes:

| Language   | Code   |
|------------|--------|
| Spanish    | `es`   |
| French     | `fr`   |
| German     | `de`   |
| Italian    | `it`   |
| Portuguese | `pt`   |
| Japanese   | `ja`   |
| Korean     | `ko`   |
| Chinese    | `zh-CN`|
| Arabic     | `ar`   |
| Hindi      | `hi`   |
| Russian    | `ru`   |

---

## Development

```bash
pip install -e ".[dev]"
pytest
```

---

## How it works

### Voice preservation
XTTS v2 is a multilingual voice-cloning TTS model.  It takes a short audio
clip of the target speaker and generates speech in any supported language that
sounds like that person speaking with a natural accentâ€”satisfying the
requirement to *keep voices the same with a bit of accent after translation*.

### Background audio preservation
Demucs (a state-of-the-art music source separation model) splits the original
audio into a vocals stem and a background stem.  After TTS synthesis the
background is re-mixed with the translated speech at a slightly lower volume
to ensure seamless transition without background sound loss.

### Lip synchronisation
Wav2Lip re-renders the mouth region of every video frame to match the new
audio, producing convincing lip sync without modifying the rest of the face or
background.

### Timing alignment
Each TTS segment is individually time-stretched using librosa's phase-vocoder
so that the translated speech fills exactly the same time window as the
original utterance.  Rate is clamped to Â±2Ã— to avoid excessive distortion.

